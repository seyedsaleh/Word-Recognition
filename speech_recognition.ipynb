{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuODsuKMdwoQ"
   },
   "source": [
    "\"In The Name Of ALLAH\"\n",
    "# Computational Intelligence Lab Project - Mr. Amini\n",
    "## Simple Word Recognition using CNN\n",
    "Seyed Mohammadsaleh Mirzatabatabaei (smsmt@aut.ac.ir) - 9623105 \n",
    "\n",
    "---\n",
    "\n",
    "![main image](https://data-flair.training/blogs/wp-content/uploads/sites/2/2019/07/speech-recognition-using-CNN.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J0w5tzoegMO"
   },
   "source": [
    "## Packages and Dependencies\n",
    "\n",
    "In order to perform this task, I'm using *Google Colab* environment, I will use *soundfile* and *librosa* libraries for soundfile processing in future and I will create the model using *keras* tensorflow API. \n",
    "\n",
    "Following libraries will be used throughout this code, make sure you’ve installed it before trying out the codes.\n",
    "**librosa, soundfile, keras, tensorflow, scikit-learn, numpy, matplotlib**\n",
    "\n",
    "So let's import packages and dependencies which we need for our word recognition engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nywkSZuJSVbG"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import sys, os\n",
    "import glob\n",
    "from IPython.display import Audio\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnndaSIaAioL",
    "outputId": "2d361c25-5169-41c7-900d-ab1022819601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ub5O5mRek7a"
   },
   "source": [
    "## Upload Train Dataset\n",
    "\n",
    "For this project there are basically six classes to recognize from. There is no problem to add as much as you may wish. You should just change in number of model category output and input dataset, labels a bit.\n",
    "The task in this project will be to classify an audio between six words in farsi language:\n",
    "*   Garm\n",
    "*   Sard\n",
    "*   Roshan\n",
    "*   Tarik\n",
    "*   Khodkar\n",
    "*   Dasti\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will use our own dataset which we have collected for this project.\n",
    "This database has been collected by **Telegram** messengers (social network), thanks its voice recording.\n",
    "First we record voice data from about 250 different people varrying the sex, age and save these data, then we categorized each voice data to its specific group (labeling) and did the task of data cleaning and converting its format from **.ogg** to **.wav**. This dataset has been prepared after several days of efforts by a group of 10 people.\n",
    "\n",
    "\n",
    "Each folder contains approximately 250 audio files for each word. The name of the folder is actually the label of those audio files. You can play some audio files randomly to get an overall idea.\n",
    "\n",
    "Before start the code you should upload your dataset: (as we use Farsi dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxVjrMpeahMM",
    "outputId": "6000650b-37bf-4b61-d9ed-b3a934d0f1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please drag and drop Dataset.zip file in the Colab files folder\n",
      "or select your Dataset (Dataset.zip):\n",
      "\n",
      "dataset files unzipped...\n"
     ]
    }
   ],
   "source": [
    "print(\"Please drag and drop Dataset.zip file in the Colab files folder\" +\n",
    "      \"\\nor select your Dataset (Dataset.zip):\\n\")\n",
    "# files_model = files.upload()\n",
    "\n",
    "labels = ['Dasti', 'Garm', 'Khodkar', 'Roshan', 'Sard', 'Tarik']\n",
    "path = '/content/'\n",
    "\n",
    "for label in labels:\n",
    "  dirpath = os.path.join(path, label)\n",
    "  if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)\n",
    "dataset = ZipFile(\"/content/drive/MyDrive/Farsi_Dataset.zip\", 'r')\n",
    "dataset.extractall('')\n",
    "dataset.close()\n",
    "print(\"dataset files unzipped...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sJsqyzKfA4V"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9rDLvoWL14w"
   },
   "source": [
    "Due to the lack of data in the collected database, we will add some *noise* to each data and use it as new data for network training. The extra noise power is calculated according to the power of each signal so that the audio signal is not completely damaged.\n",
    "\n",
    "In this way, we will have about **500** data for each word to train our word recognition network.\n",
    "We will use one 1 audio channel (mono) to read each voice. Thus, for each sound, we will have a signal with a specific sampling frequency, The sampling frequency of the input dataset signals is **54,000 Hz,** which we did not change.\n",
    "\n",
    "To clear the data, we read the zero values ​​after reading the audio signals from both sides. (trimming)\n",
    "\n",
    "Finally, we sorted data by numbers and the original audio file and the noise file are stored in a folder with the word name as the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seKCpnyngCdW",
    "outputId": "10f58a26-d9bd-4692-de35-decc6f51aacc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dasti files added...\n",
      "Garm files added...\n",
      "Khodkar files added...\n",
      "Roshan files added...\n",
      "Sard files added...\n",
      "Tarik files added...\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "  folderpath = os.path.join(path, label)\n",
    "  i = 1\n",
    "  for file in os.listdir(folderpath):\n",
    "    filename = file.split('.')[0]\n",
    "    signal, Fs = librosa.load(os.path.join(folderpath, file), mono=True, sr=None)\n",
    "    new_signal = np.trim_zeros(signal)\n",
    "\n",
    "    target_snr_db = 20\n",
    "    sigp_avg = np.mean(new_signal ** 2)\n",
    "    sigp_avg_db = 10 * np.log10(sigp_avg)\n",
    "    noise_avg_db = sigp_avg_db - target_snr_db\n",
    "    noise_avg = 10 ** (noise_avg_db / 10)\n",
    "    mean_noise = 0\n",
    "    noise = np.random.normal(mean_noise, np.sqrt(noise_avg), len(new_signal ** 2))\n",
    "    noised_signal = new_signal + noise\n",
    "    \n",
    "    os.remove(os.path.join(folderpath, file))\n",
    "    sf.write(os.path.join(folderpath, f'{i}.wav'), new_signal, Fs)\n",
    "    sf.write(os.path.join(folderpath, f'{i}_noised.wav'), noised_signal, Fs)\n",
    "    i = i + 1\n",
    "  print(f'{label} files added...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkMyhk1tfL8j"
   },
   "source": [
    "## Pre-processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRBvXV_QOWOT"
   },
   "source": [
    "> We have two key problem for preparing data to feed into the neural network.\n",
    "\n",
    "1.   We can’t just feed an audio file to a CNN, that’s outrageous!\n",
    "2.   We need to prepare a fixed size vector for each audio file for classification.\n",
    "\n",
    "> What's the solution?\n",
    "\n",
    "1.   We could use an embedding to overcome this problem, An embedding is a mapping from discrete objects, such as words vectors of real numbers.\n",
    "There are a lot of techniques and python packages for audio feature extraction. We use the most obvious and simple one which is called **MFCC encoding**, which is super effective for working with speech signals.\n",
    "\n",
    "2.   To overcome this problem all we need to do is to pad the output vectors with constant value 0.\n",
    "MFCC vectors might vary in size for different audio input, CNN can’t handle sequence data so we need to prepare a fixed size vector for all of the audio files.\n",
    "\n",
    "\n",
    "*MFCC (Mel Frequency Cepstral Coefficients): In short,In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.*\n",
    "\n",
    "\n",
    "> **Functions**\n",
    "\n",
    "*   **wav2mfcc:**  In this function we will process the signal and get its **MFCC** so finally we are pooling signal with 0 to the size of pre defined *max_width*\n",
    "\n",
    "![mfcc](https://image.slidesharecdn.com/speakerrecognitionusingmffc-131219065136-phpapp01/95/speaker-recognition-using-mfcc-8-638.jpg?cb=1403413530)\n",
    "\n",
    "*   **save_mfcc_model:**  Reads all sound files from each labeled directory and do the mfcc transform then save the mfcc created matrices in a .npy file which is named after the name of the label to use for training network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npvBQ_xlgCzB",
    "outputId": "2a9cc3d9-9c35-470d-9248-ec9398be9789"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving mfcc numpy model of word 'Dasti': 100%|██████████| 926/926 [00:13<00:00, 66.45it/s]\n",
      "Saving mfcc numpy model of word 'Garm': 100%|██████████| 908/908 [00:12<00:00, 70.92it/s]\n",
      "Saving mfcc numpy model of word 'Khodkar': 100%|██████████| 908/908 [00:13<00:00, 66.53it/s]\n",
      "Saving mfcc numpy model of word 'Roshan': 100%|██████████| 931/931 [00:13<00:00, 69.48it/s]\n",
      "Saving mfcc numpy model of word 'Sard': 100%|██████████| 922/922 [00:13<00:00, 69.03it/s]\n",
      "Saving mfcc numpy model of word 'Tarik': 100%|██████████| 908/908 [00:13<00:00, 67.70it/s]\n"
     ]
    }
   ],
   "source": [
    "def wav2mfcc(signal, Fs=54000, max_width):\n",
    "  mfcc = librosa.feature.mfcc(signal, sr=Fs)\n",
    "  mfcc = np.pad(mfcc, pad_width=((0, 0), (0, max_width - mfcc.shape[1])), mode='constant')\n",
    "  return mfcc\n",
    "\n",
    "\n",
    "def save_mfcc_model(data_path, model_path, max_width):\n",
    "  for label in labels:\n",
    "      mfcc_vectors = []\n",
    "      folderpath = os.path.join(data_path, label)\n",
    "      for file in tqdm(glob.glob(f\"{data_path}/{label}/*.wav\"), f\"Saving mfcc numpy model of word '{label}'\"):\n",
    "          signal, Fs = librosa.load(file, mono=True, sr=None)\n",
    "          mfcc = wav2mfcc(signal, Fs, max_width)\n",
    "          mfcc_vectors.append(mfcc)\n",
    "      np.save(os.path.join(model_path, f'{label}.npy'), mfcc_vectors)\n",
    "\n",
    "\n",
    "max_width = 0\n",
    "for label in labels:\n",
    "  for file in glob.glob(f\"{path}/{label}/*.wav\"):\n",
    "    signal, Fs = librosa.load(file, mono=True, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(signal, sr=Fs)\n",
    "    max_width = max(max_width, mfcc.shape[1])\n",
    "save_mfcc_model(path, path, max_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwilabu8fTVZ"
   },
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GlbQToeTiFvo"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "  def __init__(self, labels, mfcc_shape, model_path):\n",
    "    self.mfcc_shape = mfcc_shape\n",
    "    self.labels = labels\n",
    "    self.num_labels = len(labels)\n",
    "    self.model_path = model_path\n",
    "    self.model = self.get_model()\n",
    "\n",
    "\n",
    "  def get_model(self):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(2, 2), activation='relu', input_shape=(self.mfcc_shape[0], self.mfcc_shape[1], 1)))\n",
    "    model.add(Conv2D(128, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(256, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(self.num_labels, activation='softmax'))\n",
    "    model.compile(loss = keras.losses.categorical_crossentropy,\n",
    "                  optimizer = keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "  def get_data(self, model_path, test_size):\n",
    "    X = np.empty([1, self.mfcc_shape[0], self.mfcc_shape[1]])\n",
    "    y = np.array([])\n",
    "    for i, label in enumerate(self.labels):\n",
    "      label_x = np.load(f'{model_path}/{label}.npy')\n",
    "      X = np.vstack((X, label_x))\n",
    "      y = np.append(y, np.full(label_x.shape[0], fill_value=i))\n",
    "    X = np.delete(X, 0, axis=0)\n",
    "\n",
    "    return train_test_split(X, y, test_size= test_size, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "  def train(self, epochs=50, batch_size=50, verbose=1, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = self.get_data(self.model_path, test_size)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], self.mfcc_shape[0], self.mfcc_shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], self.mfcc_shape[0], self.mfcc_shape[1], 1)\n",
    "    y_train_hot = to_categorical(y_train)\n",
    "    y_test_hot = to_categorical(y_test)\n",
    "    \n",
    "    self.hist = self.model.fit(X_train, y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, y_test_hot))\n",
    "\n",
    "\n",
    "  def save(self):\n",
    "    if not os.path.exists('Model/'):\n",
    "      os.makedirs('Model/')\n",
    "    self.model.save('Model/model.h5')\n",
    "    print(\"The trained CNN model have been saved in the Model folder.\")\n",
    "    \n",
    "\n",
    "  def plot_loss(self):\n",
    "    plt.plot(self.hist.history['loss'])\n",
    "    plt.plot(self.hist.history['val_loss'])\n",
    "    plt.title(\"Model Loss per Epoch\")\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    plt.savefig('Loss_per_Epoch_final.png', transparent=True)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "  def predict(self, filepath):\n",
    "    signal, Fs = librosa.load(filepath, mono=True, sr=None)\n",
    "    new_signal = np.trim_zeros(signal)\n",
    "    sample = wav2mfcc(new_signal, self.mfcc_shape[1])\n",
    "    sample_reshaped = sample.reshape(1, self.mfcc_shape[0], self.mfcc_shape[1], 1)\n",
    "    return self.labels[np.argmax(self.model.predict(sample_reshaped))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6yBEU76fYru"
   },
   "source": [
    "## Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9wH3jH2GLfbG",
    "outputId": "7a66a8ca-a82f-4c83-c239-d9e151794682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 19, 371, 64)       320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 18, 370, 128)      32896     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 17, 369, 256)      131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 184, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 184, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 376832)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               96469248  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 96,642,214\n",
      "Trainable params: 96,642,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "45/45 [==============================] - 12s 232ms/step - loss: 7.8645 - accuracy: 0.1918 - val_loss: 0.8062 - val_accuracy: 0.7530\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 10s 227ms/step - loss: 0.9824 - accuracy: 0.6426 - val_loss: 0.1614 - val_accuracy: 0.9691\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 10s 229ms/step - loss: 0.3837 - accuracy: 0.8719 - val_loss: 0.0455 - val_accuracy: 0.9918\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 10s 232ms/step - loss: 0.2396 - accuracy: 0.9160 - val_loss: 0.0266 - val_accuracy: 0.9946\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 11s 234ms/step - loss: 0.1565 - accuracy: 0.9484 - val_loss: 0.0106 - val_accuracy: 0.9955\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 11s 235ms/step - loss: 0.1295 - accuracy: 0.9510 - val_loss: 0.0124 - val_accuracy: 0.9973\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 11s 235ms/step - loss: 0.1036 - accuracy: 0.9614 - val_loss: 0.0130 - val_accuracy: 0.9964\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 11s 237ms/step - loss: 0.0696 - accuracy: 0.9738 - val_loss: 0.0165 - val_accuracy: 0.9946\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 11s 238ms/step - loss: 0.0788 - accuracy: 0.9745 - val_loss: 0.0108 - val_accuracy: 0.9973\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 11s 240ms/step - loss: 0.0607 - accuracy: 0.9767 - val_loss: 0.0109 - val_accuracy: 0.9964\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 11s 242ms/step - loss: 0.0636 - accuracy: 0.9829 - val_loss: 0.0059 - val_accuracy: 0.9973\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 11s 243ms/step - loss: 0.0478 - accuracy: 0.9833 - val_loss: 0.0100 - val_accuracy: 0.9973\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 11s 248ms/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 0.0207 - val_accuracy: 0.9973\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 11s 250ms/step - loss: 0.0301 - accuracy: 0.9882 - val_loss: 0.0185 - val_accuracy: 0.9964\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 11s 249ms/step - loss: 0.0497 - accuracy: 0.9845 - val_loss: 0.0082 - val_accuracy: 0.9973\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 11s 247ms/step - loss: 0.0564 - accuracy: 0.9860 - val_loss: 0.0077 - val_accuracy: 0.9973\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 11s 249ms/step - loss: 0.0223 - accuracy: 0.9920 - val_loss: 0.0098 - val_accuracy: 0.9973\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 11s 247ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.0202 - val_accuracy: 0.9973\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 11s 248ms/step - loss: 0.0382 - accuracy: 0.9894 - val_loss: 0.0121 - val_accuracy: 0.9973\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 11s 249ms/step - loss: 0.0224 - accuracy: 0.9912 - val_loss: 0.0093 - val_accuracy: 0.9982\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 11s 251ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0208 - val_accuracy: 0.9973\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 11s 251ms/step - loss: 0.0281 - accuracy: 0.9922 - val_loss: 0.0128 - val_accuracy: 0.9973\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 11s 251ms/step - loss: 0.0219 - accuracy: 0.9925 - val_loss: 0.0254 - val_accuracy: 0.9973\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 11s 251ms/step - loss: 0.0243 - accuracy: 0.9934 - val_loss: 0.0279 - val_accuracy: 0.9973\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 11s 252ms/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 0.0115 - val_accuracy: 0.9973\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 11s 252ms/step - loss: 0.0175 - accuracy: 0.9925 - val_loss: 0.0324 - val_accuracy: 0.9973\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 11s 251ms/step - loss: 0.0355 - accuracy: 0.9885 - val_loss: 0.0813 - val_accuracy: 0.9700\n",
      "Epoch 28/30\n",
      "45/45 [==============================] - 11s 252ms/step - loss: 0.0582 - accuracy: 0.9830 - val_loss: 0.0517 - val_accuracy: 0.9982\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 11s 253ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.0087 - val_accuracy: 0.9973\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 11s 253ms/step - loss: 0.0192 - accuracy: 0.9952 - val_loss: 0.0136 - val_accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model(labels, (20, max_width), path)\n",
    "\n",
    "model.train(epochs=30, batch_size=100, verbose=1, test_size=0.2)\n",
    "model.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt0V8YyKTm4Z"
   },
   "source": [
    "**Final epoch:**\n",
    "\n",
    "loss: 0.0192 - accuracy: 0.9952 - val_loss: 0.0136 - val_accuracy: 0.9973\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvUovLcGfd74"
   },
   "source": [
    "## Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "p5HlO_Z2l8yX",
    "outputId": "6c1e05cb-0ba3-497b-a620-25bd33599e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-b702bafe-50a7-466e-9c7d-81d4aedc2192\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-b702bafe-50a7-466e-9c7d-81d4aedc2192\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Sard.wav to Sard.wav\n",
      "\n",
      "\n",
      "Result: Sard\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data:\\n\")\n",
    "files_model = files.upload()\n",
    "file = [file for file in files_model.keys()]\n",
    "file_path = os.path.join(path, file[0])\n",
    "\n",
    "# file_path = '/content/Garm.wav'\n",
    "result = model.predict(file_path)\n",
    "print(f'\\n\\nResult: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKkjybUsTZpk"
   },
   "source": [
    "## Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_VEKembNDlS",
    "outputId": "f08019be-c237-4a86-c5a4-c1b43baea84f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained CNN model have been saved in the Model folder.\n"
     ]
    }
   ],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF9aLzUOV2gd"
   },
   "source": [
    "## Raspberry Code\n",
    "\n",
    "Raspberry GPIO pins:\n",
    "\n",
    "![mfcc](https://projects-static.raspberrypi.org/projects/physical-computing/ddf11cf76ef996ec5fe618e1aec76008bb4b2a96/en/images/pinout.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSJdUfEQV6EI"
   },
   "outputs": [],
   "source": [
    "from gpiozero import LED\n",
    "from pyaudio import PyAudio as audio\n",
    "\n",
    "# load model\n",
    "model = load_model(\"Model/model.h5\")\n",
    "\n",
    "# audio recording setting\n",
    "form = pyaudio.paInt16\n",
    "chans = 1\n",
    "samp_rate = 54000\n",
    "chunk = 4096\n",
    "record_secs = 4     #record time\n",
    "dev_index = 2\n",
    "wav_output_filename = 'test.wav'\n",
    "\n",
    "#setup audio input stream\n",
    "stream = audio.open(format=form, rate=samp_rate, channels=chans, input_device_index=dev_index, input=True, frames_per_buffer=chunk)\n",
    "print(\"recording\")\n",
    "frames=[]\n",
    "for ii in range(0,int((samp_rate/chunk)*record_secs)):\n",
    "    data=stream.read(chunk,exception_on_overflow = False)\n",
    "    frames.append(data)\n",
    "print(\"finished recording\")\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# creates wave file with audio read in\n",
    "wavefile = wave.open(wav_output_filename, 'wb')\n",
    "wavefile.setnchannels(chans)\n",
    "wavefile.setsampwidth(audio.get_sample_size(form))\n",
    "wavefile.setframerate(samp_rate)\n",
    "wavefile.writeframes(b''.join(frames))\n",
    "wavefile.close()\n",
    "\n",
    "# Raspberry led labels\n",
    "auto_led = LED(2)\n",
    "Garm = LED(3)\n",
    "Sard = LED(4)\n",
    "Roshan = LED(17)\n",
    "Tarik = LED(27)\n",
    "\n",
    "result = model.predict(wav_output_filename)\n",
    "\n",
    "# automatic setting\n",
    "if result = 'Khodkar':\n",
    "  auto = True\n",
    "  auto_led.on()\n",
    "else if result = 'Dasti':\n",
    "  auto = False\n",
    "  auto_led.off()\n",
    "\n",
    "# actions\n",
    "if auto:\n",
    "  if result = 'Garm':\n",
    "    Sard.off()\n",
    "    Garm.on()\n",
    "  else if result = 'Sard':\n",
    "    Garm.off()\n",
    "    Sard.on()\n",
    "  else if result = 'Roshan':\n",
    "    Tarik.off()\n",
    "    Roshan.on()\n",
    "  else if result = 'Tarik':\n",
    "    Roshan.off()\n",
    "    Tarik.on()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "speech recognition.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
